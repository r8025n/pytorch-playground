{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.3.1\n",
      "torchvision version: 0.18.1a0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\\ntorchvision version: {torchvision.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datasets.FashionMNIST(\n",
    "    root=\"./data/\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    "    target_transform=None\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"./data/\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000, 0.0510,\n",
       "          0.2863, 0.0000, 0.0000, 0.0039, 0.0157, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0039, 0.0039, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.0000, 0.1412, 0.5333,\n",
       "          0.4980, 0.2431, 0.2118, 0.0000, 0.0000, 0.0000, 0.0039, 0.0118,\n",
       "          0.0157, 0.0000, 0.0000, 0.0118],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0235, 0.0000, 0.4000, 0.8000,\n",
       "          0.6902, 0.5255, 0.5647, 0.4824, 0.0902, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0471, 0.0392, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6078, 0.9255,\n",
       "          0.8118, 0.6980, 0.4196, 0.6118, 0.6314, 0.4275, 0.2510, 0.0902,\n",
       "          0.3020, 0.5098, 0.2824, 0.0588],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.2706, 0.8118, 0.8745,\n",
       "          0.8549, 0.8471, 0.8471, 0.6392, 0.4980, 0.4745, 0.4784, 0.5725,\n",
       "          0.5529, 0.3451, 0.6745, 0.2588],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0039, 0.0039, 0.0039, 0.0000, 0.7843, 0.9098, 0.9098,\n",
       "          0.9137, 0.8980, 0.8745, 0.8745, 0.8431, 0.8353, 0.6431, 0.4980,\n",
       "          0.4824, 0.7686, 0.8980, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7176, 0.8824, 0.8471,\n",
       "          0.8745, 0.8941, 0.9216, 0.8902, 0.8784, 0.8706, 0.8784, 0.8667,\n",
       "          0.8745, 0.9608, 0.6784, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7569, 0.8941, 0.8549,\n",
       "          0.8353, 0.7765, 0.7059, 0.8314, 0.8235, 0.8275, 0.8353, 0.8745,\n",
       "          0.8627, 0.9529, 0.7922, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0039, 0.0118, 0.0000, 0.0471, 0.8588, 0.8627, 0.8314,\n",
       "          0.8549, 0.7529, 0.6627, 0.8902, 0.8157, 0.8549, 0.8784, 0.8314,\n",
       "          0.8863, 0.7725, 0.8196, 0.2039],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0235, 0.0000, 0.3882, 0.9569, 0.8706, 0.8627,\n",
       "          0.8549, 0.7961, 0.7765, 0.8667, 0.8431, 0.8353, 0.8706, 0.8627,\n",
       "          0.9608, 0.4667, 0.6549, 0.2196],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0157, 0.0000, 0.0000, 0.2157, 0.9255, 0.8941, 0.9020,\n",
       "          0.8941, 0.9412, 0.9098, 0.8353, 0.8549, 0.8745, 0.9176, 0.8510,\n",
       "          0.8510, 0.8196, 0.3608, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0039, 0.0157, 0.0235, 0.0275, 0.0078, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.9294, 0.8863, 0.8510, 0.8745,\n",
       "          0.8706, 0.8588, 0.8706, 0.8667, 0.8471, 0.8745, 0.8980, 0.8431,\n",
       "          0.8549, 1.0000, 0.3020, 0.0000],\n",
       "         [0.0000, 0.0118, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.2431, 0.5686, 0.8000, 0.8941, 0.8118, 0.8353, 0.8667,\n",
       "          0.8549, 0.8157, 0.8275, 0.8549, 0.8784, 0.8745, 0.8588, 0.8431,\n",
       "          0.8784, 0.9569, 0.6235, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.1725, 0.3216, 0.4196,\n",
       "          0.7412, 0.8941, 0.8627, 0.8706, 0.8510, 0.8863, 0.7843, 0.8039,\n",
       "          0.8275, 0.9020, 0.8784, 0.9176, 0.6902, 0.7373, 0.9804, 0.9725,\n",
       "          0.9137, 0.9333, 0.8431, 0.0000],\n",
       "         [0.0000, 0.2235, 0.7333, 0.8157, 0.8784, 0.8667, 0.8784, 0.8157,\n",
       "          0.8000, 0.8392, 0.8157, 0.8196, 0.7843, 0.6235, 0.9608, 0.7569,\n",
       "          0.8078, 0.8745, 1.0000, 1.0000, 0.8667, 0.9176, 0.8667, 0.8275,\n",
       "          0.8627, 0.9098, 0.9647, 0.0000],\n",
       "         [0.0118, 0.7922, 0.8941, 0.8784, 0.8667, 0.8275, 0.8275, 0.8392,\n",
       "          0.8039, 0.8039, 0.8039, 0.8627, 0.9412, 0.3137, 0.5882, 1.0000,\n",
       "          0.8980, 0.8667, 0.7373, 0.6039, 0.7490, 0.8235, 0.8000, 0.8196,\n",
       "          0.8706, 0.8941, 0.8824, 0.0000],\n",
       "         [0.3843, 0.9137, 0.7765, 0.8235, 0.8706, 0.8980, 0.8980, 0.9176,\n",
       "          0.9765, 0.8627, 0.7608, 0.8431, 0.8510, 0.9451, 0.2549, 0.2863,\n",
       "          0.4157, 0.4588, 0.6588, 0.8588, 0.8667, 0.8431, 0.8510, 0.8745,\n",
       "          0.8745, 0.8784, 0.8980, 0.1137],\n",
       "         [0.2941, 0.8000, 0.8314, 0.8000, 0.7569, 0.8039, 0.8275, 0.8824,\n",
       "          0.8471, 0.7255, 0.7725, 0.8078, 0.7765, 0.8353, 0.9412, 0.7647,\n",
       "          0.8902, 0.9608, 0.9373, 0.8745, 0.8549, 0.8314, 0.8196, 0.8706,\n",
       "          0.8627, 0.8667, 0.9020, 0.2627],\n",
       "         [0.1882, 0.7961, 0.7176, 0.7608, 0.8353, 0.7725, 0.7255, 0.7451,\n",
       "          0.7608, 0.7529, 0.7922, 0.8392, 0.8588, 0.8667, 0.8627, 0.9255,\n",
       "          0.8824, 0.8471, 0.7804, 0.8078, 0.7294, 0.7098, 0.6941, 0.6745,\n",
       "          0.7098, 0.8039, 0.8078, 0.4510],\n",
       "         [0.0000, 0.4784, 0.8588, 0.7569, 0.7020, 0.6706, 0.7176, 0.7686,\n",
       "          0.8000, 0.8235, 0.8353, 0.8118, 0.8275, 0.8235, 0.7843, 0.7686,\n",
       "          0.7608, 0.7490, 0.7647, 0.7490, 0.7765, 0.7529, 0.6902, 0.6118,\n",
       "          0.6549, 0.6941, 0.8235, 0.3608],\n",
       "         [0.0000, 0.0000, 0.2902, 0.7412, 0.8314, 0.7490, 0.6863, 0.6745,\n",
       "          0.6863, 0.7098, 0.7255, 0.7373, 0.7412, 0.7373, 0.7569, 0.7765,\n",
       "          0.8000, 0.8196, 0.8235, 0.8235, 0.8275, 0.7373, 0.7373, 0.7608,\n",
       "          0.7529, 0.8471, 0.6667, 0.0000],\n",
       "         [0.0078, 0.0000, 0.0000, 0.0000, 0.2588, 0.7843, 0.8706, 0.9294,\n",
       "          0.9373, 0.9490, 0.9647, 0.9529, 0.9569, 0.8667, 0.8627, 0.7569,\n",
       "          0.7490, 0.7020, 0.7137, 0.7137, 0.7098, 0.6902, 0.6510, 0.6588,\n",
       "          0.3882, 0.2275, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1569,\n",
       "          0.2392, 0.1725, 0.2824, 0.1608, 0.1373, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image, label = train_data[0]\n",
    "# image\n",
    "# label\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T-shirt/top',\n",
       " 'Trouser',\n",
       " 'Pullover',\n",
       " 'Dress',\n",
       " 'Coat',\n",
       " 'Sandal',\n",
       " 'Shirt',\n",
       " 'Sneaker',\n",
       " 'Bag',\n",
       " 'Ankle boot']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: torch.Size([1, 28, 28])\n",
      "Squeezed Image shape: torch.Size([28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "image, label = train_data[0]\n",
    "print(f\"Image shape: {image.shape}\")\n",
    "print(f\"Squeezed Image shape: {image.squeeze().shape}\")\n",
    "plt.imshow(image.squeeze(), cmap=\"gray\") # image shape is [1, 28, 28] (colour channels, height, width)\n",
    "plt.title(label);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "train_features_batch, train_label_batch = next(iter(train_dataloader))\n",
    "train_features_batch.shape, train_label_batch.shape\n",
    "print(len(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0078, 0.0078,\n",
      "         0.0000, 0.0000, 0.0000, 0.7294, 0.8471, 0.8431, 0.8627, 0.9686, 0.0000,\n",
      "         0.0000, 0.0000, 0.0078, 0.0078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0078, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.3922, 0.9020, 0.9176, 0.9490, 0.9333, 0.9255,\n",
      "         0.6627, 0.0000, 0.0000, 0.0000, 0.0000, 0.0157, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000,\n",
      "         0.0000, 0.3294, 0.7216, 0.9647, 0.9216, 0.8510, 0.8667, 0.8667, 0.8588,\n",
      "         0.8431, 0.9059, 1.0000, 0.7294, 0.3686, 0.0000, 0.0000, 0.0039, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0157, 0.0000,\n",
      "         0.0588, 0.7725, 0.9137, 0.8824, 0.8471, 0.8275, 0.8510, 0.9020, 0.8745,\n",
      "         0.8706, 0.8745, 0.8353, 0.8392, 0.8863, 0.9059, 0.8157, 0.2039, 0.0000,\n",
      "         0.0157, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.6863, 0.8980, 0.8039, 0.8235, 0.8275, 0.8353, 0.8157, 0.8353,\n",
      "         0.8353, 0.8314, 0.8157, 0.8353, 0.8314, 0.8275, 0.8118, 0.8745, 0.7961,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0941, 0.9020, 0.8196, 0.8235, 0.8392, 0.8314, 0.8314, 0.8275,\n",
      "         0.8431, 0.8314, 0.8235, 0.8275, 0.8353, 0.8392, 0.8353, 0.8353, 0.8157,\n",
      "         0.9176, 0.2941, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.2941, 0.9333, 0.8314, 0.8275, 0.8275, 0.8235, 0.8275,\n",
      "         0.8235, 0.8627, 0.8392, 0.8314, 0.8275, 0.8275, 0.8314, 0.8353, 0.8314,\n",
      "         0.8275, 0.9373, 0.5373, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.5373, 0.9412, 0.8667, 0.8392, 0.8314, 0.8235,\n",
      "         0.8275, 0.8196, 0.8510, 0.8078, 0.8314, 0.8275, 0.8314, 0.8431, 0.8471,\n",
      "         0.8431, 0.8667, 0.9333, 0.6863, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.7098, 0.9255, 0.8941, 0.8471, 0.8275,\n",
      "         0.8392, 0.8353, 0.8314, 0.8471, 0.8196, 0.8392, 0.8314, 0.8275, 0.8431,\n",
      "         0.8510, 0.8392, 0.8941, 0.9176, 0.8510, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.8588, 0.9020, 0.9216, 0.8549,\n",
      "         0.8392, 0.8431, 0.8353, 0.8431, 0.8588, 0.8392, 0.8510, 0.8431, 0.8431,\n",
      "         0.8627, 0.8353, 0.8510, 0.9294, 0.8902, 0.8196, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9686, 0.8902, 0.9255,\n",
      "         0.9098, 0.8078, 0.8431, 0.8392, 0.8392, 0.8627, 0.8353, 0.8392, 0.8510,\n",
      "         0.8510, 0.8588, 0.8078, 0.9059, 0.9373, 0.8863, 0.8667, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.8706, 0.8824,\n",
      "         0.9176, 0.9765, 0.8392, 0.8392, 0.8431, 0.8392, 0.8745, 0.8627, 0.8510,\n",
      "         0.8549, 0.8667, 0.8588, 0.8314, 0.9647, 0.9255, 0.8745, 0.8941, 0.0510,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0667, 0.9020,\n",
      "         0.8706, 0.9608, 0.8196, 0.8039, 0.8549, 0.8549, 0.8549, 0.8627, 0.8314,\n",
      "         0.8471, 0.8510, 0.8471, 0.8941, 0.8118, 0.8039, 0.9647, 0.8667, 0.9216,\n",
      "         0.2118, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2118,\n",
      "         0.9176, 0.8784, 0.9412, 0.6549, 0.8235, 0.8549, 0.8588, 0.8471, 0.8667,\n",
      "         0.8392, 0.8510, 0.8510, 0.8275, 0.8627, 0.8471, 0.6000, 1.0000, 0.8745,\n",
      "         0.9255, 0.3059, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.3333, 0.9176, 0.8941, 0.9961, 0.5216, 0.8549, 0.8471, 0.8706, 0.8431,\n",
      "         0.8745, 0.8510, 0.8588, 0.8510, 0.8471, 0.8275, 0.8902, 0.5490, 1.0000,\n",
      "         0.8941, 0.9216, 0.3882, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.4235, 0.9020, 0.9137, 0.9765, 0.4118, 0.8824, 0.8588, 0.8627,\n",
      "         0.8353, 0.8784, 0.8431, 0.8549, 0.8510, 0.8627, 0.8275, 0.9020, 0.4196,\n",
      "         0.9490, 0.9137, 0.9059, 0.4902, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.5647, 0.8824, 0.9333, 0.9412, 0.2784, 0.9451, 0.8510,\n",
      "         0.8431, 0.8353, 0.8706, 0.8196, 0.8627, 0.8588, 0.8510, 0.8314, 0.9647,\n",
      "         0.2863, 0.9098, 0.9373, 0.8824, 0.6196, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.6980, 0.8549, 0.9373, 0.8980, 0.2431, 0.9765,\n",
      "         0.8471, 0.8353, 0.8549, 0.8745, 0.8353, 0.8549, 0.8627, 0.8471, 0.8275,\n",
      "         1.0000, 0.2471, 0.8745, 0.9490, 0.8549, 0.6980, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.7529, 0.8431, 0.9529, 0.8471, 0.2078,\n",
      "         1.0000, 0.8471, 0.8314, 0.8706, 0.8824, 0.8549, 0.8471, 0.8667, 0.8549,\n",
      "         0.8275, 1.0000, 0.2078, 0.8275, 0.9647, 0.8353, 0.7647, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7608, 0.8314, 0.9569, 0.7882,\n",
      "         0.1569, 1.0000, 0.8275, 0.8235, 0.8706, 0.8667, 0.8275, 0.8392, 0.8549,\n",
      "         0.8510, 0.8314, 1.0000, 0.2118, 0.7725, 0.9608, 0.8314, 0.7647, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7373, 0.8314, 0.9608,\n",
      "         0.5961, 0.1529, 1.0000, 0.8314, 0.8314, 0.8667, 0.8784, 0.8392, 0.8431,\n",
      "         0.8471, 0.8627, 0.8471, 1.0000, 0.2863, 0.6000, 0.9569, 0.8353, 0.7255,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7098, 0.8353,\n",
      "         0.9765, 0.3725, 0.3529, 1.0000, 0.8275, 0.8392, 0.8627, 0.8863, 0.8549,\n",
      "         0.8549, 0.8549, 0.8627, 0.8392, 1.0000, 0.3373, 0.4157, 0.9686, 0.8314,\n",
      "         0.6941, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6627,\n",
      "         0.8431, 0.9804, 0.1804, 0.4549, 1.0000, 0.8235, 0.8353, 0.8549, 0.8784,\n",
      "         0.8353, 0.8471, 0.8471, 0.8588, 0.8353, 1.0000, 0.3961, 0.2353, 0.9882,\n",
      "         0.8471, 0.6196, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.5843, 0.8588, 0.9490, 0.0000, 0.5255, 0.8980, 0.8235, 0.8431, 0.8549,\n",
      "         0.8745, 0.8353, 0.8510, 0.8510, 0.8510, 0.8353, 0.9020, 0.4706, 0.0549,\n",
      "         0.9569, 0.8588, 0.5451, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.4510, 0.8902, 0.8941, 0.0000, 0.6392, 0.9333, 0.8157, 0.8353,\n",
      "         0.8353, 0.8392, 0.8196, 0.8275, 0.8275, 0.8353, 0.8235, 0.9412, 0.5490,\n",
      "         0.0000, 0.9216, 0.8745, 0.4000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.3882, 0.8863, 0.8431, 0.0000, 0.1765, 0.9333, 0.8863,\n",
      "         0.9137, 0.9176, 0.9176, 0.8980, 0.9059, 0.9098, 0.9176, 0.8824, 0.9373,\n",
      "         0.0314, 0.0000, 0.8706, 0.8706, 0.3294, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.6314, 0.9373, 0.8588, 0.0000, 0.0000, 0.0000,\n",
      "         0.1059, 0.2627, 0.3451, 0.4157, 0.5137, 0.4196, 0.3608, 0.2471, 0.0902,\n",
      "         0.0000, 0.0000, 0.0000, 0.9059, 0.9333, 0.5098, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.1176, 0.3608, 0.3333, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.4510, 0.4784, 0.1333, 0.0000, 0.0000,\n",
      "         0.0000]])\n"
     ]
    }
   ],
   "source": [
    "flatten_model = nn.Flatten()\n",
    "\n",
    "test_data = train_features_batch[0]\n",
    "print(test_data.shape)\n",
    "test_output = flatten_model(test_data)\n",
    "print(test_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionMINSTModelv0(nn.Module):\n",
    "    def __init__(self, input_shape, hidden_units, output_shape):\n",
    "        super().__init__()\n",
    "        self.layer_stack = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=input_shape, out_features=hidden_units),\n",
    "            nn.Linear(in_features=hidden_units, out_features=output_shape)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer_stack(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionMINSTModelv1(nn.Module):\n",
    "    def __init__(self, input_shape, hidden_units, output_shape):\n",
    "        super().__init__()\n",
    "        self.layer_stack = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=input_shape, out_features=hidden_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=hidden_units, out_features=output_shape),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer_stack(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a convolutional neural network \n",
    "class FashionMNISTModelv2(nn.Module):\n",
    "    \"\"\"\n",
    "    Model architecture copying TinyVGG from: ,.\n",
    "    https://poloclub.github.io/cnn-explainer/\n",
    "    \"\"\"\n",
    "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
    "        super().__init__()\n",
    "        self.block_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input_shape, \n",
    "                      out_channels=hidden_units, \n",
    "                      kernel_size=3, # how big is the square that's going over the image?\n",
    "                      stride=1, # default\n",
    "                      padding=1),# options = \"valid\" (no padding) or \"same\" (output has same shape as input) or int for specific number \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=hidden_units, \n",
    "                      out_channels=hidden_units,\n",
    "                      kernel_size=3,\n",
    "                      stride=1,\n",
    "                      padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2,\n",
    "                         stride=2) # default stride value is same as kernel_size\n",
    "        )\n",
    "        self.block_2 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_units, hidden_units, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_units, hidden_units, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            # Where did this in_features shape come from? \n",
    "            # It's because each layer of our network compresses and changes the shape of our inputs data.\n",
    "            nn.Linear(in_features=hidden_units*7*7, \n",
    "                      out_features=output_shape)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.block_1(x)\n",
    "        # print(x.shape)\n",
    "        x = self.block_2(x)\n",
    "        # print(x.shape)\n",
    "        x = self.classifier(x)\n",
    "        # print(x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0 = FashionMINSTModelv0(input_shape=test_output.shape[1], hidden_units=10, output_shape=len(train_data.classes))\n",
    "model_1 = FashionMINSTModelv1(input_shape=test_output.shape[1], hidden_units=10, output_shape=len(train_data.classes))\n",
    "model_2 = FashionMNISTModelv2(input_shape=1, hidden_units=10, output_shape=len(train_data.classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import accuracy_fn # Note: could also use torchmetrics.Accuracy(task = 'multiclass', num_classes=len(class_names)).to(device)\n",
    "\n",
    "# Setup loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss() # this is also called \"criterion\"/\"cost function\" in some places\n",
    "optimizer_0 = torch.optim.SGD(params=model_0.parameters(), lr=0.1)\n",
    "optimizer_1 = torch.optim.SGD(params=model_1.parameters(), lr=0.1)\n",
    "optimizer_2 = torch.optim.SGD(params=model_2.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "------\n",
      "Looked at 0/60000 samples\n",
      "Looked at 12800/60000 samples\n",
      "Looked at 25600/60000 samples\n",
      "Looked at 38400/60000 samples\n",
      "Looked at 51200/60000 samples\n",
      "\n",
      "Train loss: 0.58662 | Test loss: 0.39376, Test acc: 85.65%\n",
      "\n",
      "Epoch: 1\n",
      "------\n",
      "Looked at 0/60000 samples\n",
      "Looked at 12800/60000 samples\n",
      "Looked at 25600/60000 samples\n",
      "Looked at 38400/60000 samples\n",
      "Looked at 51200/60000 samples\n",
      "\n",
      "Train loss: 0.34816 | Test loss: 0.33959, Test acc: 87.72%\n",
      "\n",
      "Epoch: 2\n",
      "------\n",
      "Looked at 0/60000 samples\n",
      "Looked at 12800/60000 samples\n",
      "Looked at 25600/60000 samples\n",
      "Looked at 38400/60000 samples\n",
      "Looked at 51200/60000 samples\n",
      "\n",
      "Train loss: 0.30969 | Test loss: 0.32405, Test acc: 88.17%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "epochs = 3\n",
    "model = model_2\n",
    "optimizer = optimizer_2\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch: {epoch}\\n------\")\n",
    "    train_loss = 0\n",
    "\n",
    "    for batch, (X, y) in enumerate(train_dataloader):\n",
    "        model.train()\n",
    "\n",
    "        y_pred = model(X)\n",
    "        \n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 400 == 0:\n",
    "            print(f\"Looked at {batch * len(X)}/{len(train_dataloader.dataset)} samples\")\n",
    "\n",
    "    train_loss /= len(train_dataloader)\n",
    "\n",
    "    test_loss, test_acc = 0, 0 \n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for (X_test, y_test) in test_dataloader:\n",
    "            # 1. Forward pass\n",
    "            test_pred = model(X_test)\n",
    "           \n",
    "            # 2. Calculate loss (accumatively)\n",
    "            test_loss += loss_fn(test_pred, y_test) # accumulatively add up the loss per epoch\n",
    "\n",
    "            # 3. Calculate accuracy (preds need to be same as y_true)\n",
    "            test_acc += accuracy_fn(y_true=y_test, y_pred=test_pred.argmax(dim=1))\n",
    "        \n",
    "        # Calculations on test metrics need to happen inside torch.inference_mode()\n",
    "        # Divide total test loss by length of test dataloader (per batch)\n",
    "        test_loss /= len(test_dataloader)\n",
    "\n",
    "        # Divide total accuracy by length of test dataloader (per batch)\n",
    "        test_acc /= len(test_dataloader)\n",
    "\n",
    "    ## Print out what's happening\n",
    "    print(f\"\\nTrain loss: {train_loss:.5f} | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
